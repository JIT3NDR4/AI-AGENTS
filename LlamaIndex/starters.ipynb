{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2474ced6-4339-4d36-ba66-18bdb8e2124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface-api\n",
      "  Downloading llama_index_llms_huggingface_api-0.6.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.6.0-py3-none-any.whl.metadata (458 bytes)\n",
      "Collecting huggingface-hub>=0.31.0 (from llama-index-llms-huggingface-api)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting llama-index-core<0.14,>=0.13.0 (from llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_core-0.13.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting aiohttp<4,>=3.8.6 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached aiohttp-3.12.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting aiosqlite (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: httpx in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.28.1)\n",
      "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_workflows-1.3.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (4.3.8)\n",
      "Collecting pydantic>=2.8.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.32.5)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sqlalchemy>=1.4.49 (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached sqlalchemy-2.0.43-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.7.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm<5,>=4.66.1 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (4.14.1)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading griffe-1.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.1.6)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_instrumentation-0.4.0-py3-none-any.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.10)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (25.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api)\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-macosx_11_0_arm64.whl.metadata (703 bytes)\n",
      "Collecting click (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached regex-2025.7.34-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (2025.8.3)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading torch-2.8.0-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jitendra_vr/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface-api) (3.0.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading llama_index_llms_huggingface_api-0.6.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading llama_index_core-0.13.2-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.12.15-cp311-cp311-macosx_11_0_arm64.whl (471 kB)\n",
      "Downloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_workflows-1.3.0-py3-none-any.whl (42 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl (89 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.6.0-py3-none-any.whl (8.9 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl (47 kB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_instrumentation-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached regex-2025.7.34-cp311-cp311-macosx_11_0_arm64.whl (285 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sqlalchemy-2.0.43-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Downloading greenlet-3.2.4-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-macosx_11_0_arm64.whl (999 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.3/999.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.8.0-cp311-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading griffe-1.12.1-py3-none-any.whl (138 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, filetype, dirtyjson, wrapt, typing-inspection, tqdm, threadpoolctl, tenacity, sympy, sqlalchemy, setuptools, safetensors, regex, pydantic-core, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, hf-xet, greenlet, fsspec, frozenlist, filelock, colorama, click, annotated-types, aiosqlite, aiohappyeyeballs, yarl, typing-inspect, torch, tiktoken, scipy, pydantic, nltk, huggingface-hub, griffe, deprecated, aiosignal, tokenizers, scikit-learn, llama-index-instrumentation, dataclasses-json, banks, aiohttp, transformers, llama-index-workflows, sentence-transformers, llama-index-core, llama-index-llms-huggingface-api, llama-index-embeddings-huggingface\n",
      "\u001b[2K  Attempting uninstall: setuptools8;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/55\u001b[0m [sqlalchemy]\n",
      "\u001b[2K    Found existing installation: setuptools 80.4.0\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/55\u001b[0m [sqlalchemy]\n",
      "\u001b[2K    Uninstalling setuptools-80.4.0:49;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/55\u001b[0m [sqlalchemy]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.4.00m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/55\u001b[0m [sqlalchemy]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55/55\u001b[0m [llama-index-embeddings-huggingface]55\u001b[0m [llama-index-core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 aiosqlite-0.21.0 annotated-types-0.7.0 banks-2.2.0 click-8.2.1 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filelock-3.19.1 filetype-1.2.0 frozenlist-1.7.0 fsspec-2025.7.0 greenlet-3.2.4 griffe-1.12.1 hf-xet-1.1.8 huggingface-hub-0.34.4 joblib-1.5.1 llama-index-core-0.13.2 llama-index-embeddings-huggingface-0.6.0 llama-index-instrumentation-0.4.0 llama-index-llms-huggingface-api-0.6.0 llama-index-workflows-1.3.0 marshmallow-3.26.1 mpmath-1.3.0 multidict-6.6.4 mypy-extensions-1.1.0 networkx-3.5 nltk-3.9.1 numpy-2.3.2 pillow-11.3.0 propcache-0.3.2 pydantic-2.11.7 pydantic-core-2.33.2 regex-2025.7.34 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 setuptools-80.9.0 sqlalchemy-2.0.43 sympy-1.14.0 tenacity-9.1.2 threadpoolctl-3.6.0 tiktoken-0.11.0 tokenizers-0.21.4 torch-2.8.0 tqdm-4.67.1 transformers-4.55.2 typing-inspect-0.9.0 typing-inspection-0.4.1 wrapt-1.17.3 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4902df4-4ef3-43d5-8683-c80475c47695",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-68a63485-5575d2e754364b1a7970596d;3fc58f62-4bb2-4b4c-b0c3-a44c55c2e234)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      9\u001b[39m hf_token = \u001b[33m\"\u001b[39m\u001b[33mhf_OygsELRibjmeFlQsIAoAoKMzFldwSJUTeE\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m llm = HuggingFaceInferenceAPI(\n\u001b[32m     12\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-Coder-32B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     provider=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, how are you?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# I am good, how can I help you today?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/llama_index_instrumentation/dispatcher.py:317\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    320\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/llama_index/llms/huggingface_api/base.py:314\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcomplete\u001b[39m(\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs: Any\n\u001b[32m    312\u001b[39m ) -> CompletionResponse:\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task == \u001b[33m\"\u001b[39m\u001b[33mconversational\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         chat_resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessageRole\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUSER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_resp)\n\u001b[32m    319\u001b[39m     model_kwargs = \u001b[38;5;28mself\u001b[39m._get_model_kwargs(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/llama_index_instrumentation/dispatcher.py:317\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    320\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/llama_index/llms/huggingface_api/base.py:287\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task == \u001b[33m\"\u001b[39m\u001b[33mconversational\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    285\u001b[39m     model_kwargs = \u001b[38;5;28mself\u001b[39m._get_model_kwargs(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     output: ChatCompletionOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sync_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_huggingface_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     content = output.choices[\u001b[32m0\u001b[39m].message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    293\u001b[39m     tool_calls = output.choices[\u001b[32m0\u001b[39m].message.tool_calls \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:923\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    895\u001b[39m parameters = {\n\u001b[32m    896\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    897\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    914\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    915\u001b[39m }\n\u001b[32m    916\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    917\u001b[39m     inputs=messages,\n\u001b[32m    918\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    921\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    922\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:279\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AI-AGENTS/venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-68a63485-5575d2e754364b1a7970596d;3fc58f62-4bb2-4b4c-b0c3-a44c55c2e234)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load the .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# Retrieve HF_TOKEN from the environment variables\n",
    "hf_token = \"\"\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=hf_token,\n",
    "    provider=\"auto\"\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Hello, how are you?\")\n",
    "print(response)\n",
    "# I am good, how can I help you today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8eb38-38a9-4e03-9b02-35f2a418e956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
